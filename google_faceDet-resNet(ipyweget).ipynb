{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from typing import Tuple, Union\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"{os.getcwd()}/models\"\n",
    "model_dir\n",
    "\n",
    "# Class names corresponding to the output classes\n",
    "class_names = ['close', 'open'] # 0: close, 1: open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudic/anaconda3/envs/ecg-classification/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/claudic/anaconda3/envs/ecg-classification/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 2  # Ensure this matches the number of classes in your dataset\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model.load_state_dict(torch.load(f'{model_dir}/resnet18-e_20-d_10k.pth', map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match the input size of the model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Same normalization as training\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalized_to_pixel_coordinates(\n",
    "    normalized_x: float, normalized_y: float, image_width: int,\n",
    "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
    "\n",
    "  # Checks if the float value is between 0 and 1.\n",
    "  def is_valid_normalized_value(value: float) -> bool:\n",
    "    return (value > 0 or math.isclose(0, value)) and (value < 1 or\n",
    "                                                      math.isclose(1, value))\n",
    "\n",
    "  if not (is_valid_normalized_value(normalized_x) and\n",
    "          is_valid_normalized_value(normalized_y)):\n",
    "    # TODO: Draw coordinates even if it's outside of the image bounds.\n",
    "    return None\n",
    "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "  return x_px, y_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721460776.273489   35376 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1721460776.289668   35419 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA GeForce GTX 1650 with Max-Q Design/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1721460776.320294   35422 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "base_options = python.BaseOptions(model_asset_path='models/detector.tflite')\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "detector = vision.FaceDetector.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eyes_detection(frame)-> np.ndarray:\n",
    "    \"\"\"Detect eyes in the given frame.\"\"\"\n",
    "    lefteye_img = None\n",
    "    righteye_img = None\n",
    "    image = None\n",
    "\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "    if image is None:\n",
    "        return lefteye_img, righteye_img\n",
    "    \n",
    "    detection_result = detector.detect(image)\n",
    "\n",
    "    image = np.copy(image.numpy_view())\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    \n",
    "    for detection in detection_result.detections:\n",
    "\n",
    "        keypoint_px_right = _normalized_to_pixel_coordinates(detection.keypoints[0].x, detection.keypoints[0].y, width, height)\n",
    "        if keypoint_px_right is None:\n",
    "            return lefteye_img, None\n",
    "        righteye_img = image[keypoint_px_right[1] - 40:keypoint_px_right[1] + 30, keypoint_px_right[0] - 35:keypoint_px_right[0] + 35]\n",
    "        \n",
    "        keypoint_px_left = _normalized_to_pixel_coordinates(detection.keypoints[1].x, detection.keypoints[1].y, width, height)\n",
    "        if keypoint_px_left is None:\n",
    "            return None, righteye_img\n",
    "        lefteye_img = image[keypoint_px_left[1] - 40:keypoint_px_left[1] + 30, keypoint_px_left[0] - 35:keypoint_px_left[0] + 35]\n",
    "\n",
    "    return lefteye_img, righteye_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frame(frame):\n",
    "    \"\"\"Predict the state of eyes in the given frame.\"\"\"\n",
    "\n",
    "    # Detect eyes in the frame\n",
    "    left_img, right_img = None, None\n",
    "    left_img, right_img = eyes_detection(frame)\n",
    "\n",
    "    # Check if eyes were detected\n",
    "    if left_img is None or right_img is None:\n",
    "        return \"Eyes not detected\", None, None\n",
    "\n",
    "    # Preprocess the LEFT eye image\n",
    "    left_img = transform(Image.fromarray(left_img))\n",
    "    right_img = transform(Image.fromarray(right_img))\n",
    "\n",
    "    left_img = left_img.unsqueeze(0)  # Add a batch dimension\n",
    "    right_img = right_img.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Move the images to the device\n",
    "    left_img = left_img.to(device)\n",
    "    right_img = right_img.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs_left = model(left_img)\n",
    "        _, predicted_left = torch.max(outputs_left, 1)\n",
    "        outputs_right = model(right_img)\n",
    "        _, predicted_right = torch.max(outputs_right, 1)\n",
    "\n",
    "    # Convert prediction to class label\n",
    "    if predicted_left.item() + predicted_right.item() == 0:\n",
    "        result = \"Close 2 eyes\"\n",
    "    elif predicted_left.item() + predicted_right.item() == 1:\n",
    "        result = \"Close 1 eye\"\n",
    "    else:\n",
    "        result = \"Open 2 eyes\"\n",
    "\n",
    "    return result, left_img, right_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(2, cv2.CAP_V4L2)\n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))\n",
    "width = 640\n",
    "height = 480\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the camera.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create widgets for displaying results\n",
    "result_text = widgets.Label(style={'font-size': '30px', 'color': 'red'})\n",
    "computeSpeed_text = widgets.Label(style={'font-size': '30px'})\n",
    "image_widget = widgets.Image(format='jpeg', width=640, height=480)\n",
    "left_eye_image = widgets.Image(format='jpeg')\n",
    "right_eye_image = widgets.Image(format='jpeg')\n",
    "\n",
    "right_eye_widget = widgets.VBox([\n",
    "    widgets.Label(value='Right Eye', layout=widgets.Layout(align_items='center')),\n",
    "    right_eye_image\n",
    "])\n",
    "\n",
    "left_eye_widget = widgets.VBox([\n",
    "    widgets.Label(value='Left Eye', layout=widgets.Layout(align_items='center')),\n",
    "    left_eye_image\n",
    "])\n",
    "\n",
    "output_widget = widgets.HBox([\n",
    "    image_widget,\n",
    "    widgets.VBox([widgets.VBox([result_text, computeSpeed_text]),\n",
    "                widgets.HBox([left_eye_widget, right_eye_widget])\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e557ff3e92b496dbdf869df3499c00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', height='480', width='640'), VBox(children=(VBox(children=(Label…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudic/anaconda3/envs/ecg-classification/lib/python3.9/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Display the widgets\n",
    "\n",
    "display(output_widget)\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "\n",
    "        # Predict the class of the frame\n",
    "        if len(frame.shape) == 3:\n",
    "            predicted_class, left_img, right_img = predict_frame(frame)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Calculate the frames per second (FPS)\n",
    "        end_time = time.time()\n",
    "        compute_time = end_time - start_time\n",
    "        start_time = end_time\n",
    "\n",
    "        # Update widget content\n",
    "        result_text.value = f'Predicted: {predicted_class}'\n",
    "        computeSpeed_text.value = f'Compute Time: {compute_time:.3f} s'\n",
    "\n",
    "        _, frame_buffer = cv2.imencode('.jpeg', frame)\n",
    "        image_widget.value = frame_buffer.tobytes()\n",
    "\n",
    "        if left_img is not None:\n",
    "            left_img_np = left_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            left_img_np = (left_img_np * 255).astype(np.uint8)\n",
    "            _, left_buffer = cv2.imencode('.jpeg', left_img_np)\n",
    "            left_eye_image.value = left_buffer.tobytes()\n",
    "\n",
    "        if right_img is not None:\n",
    "            right_img_np = right_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            right_img_np = (right_img_np * 255).astype(np.uint8)\n",
    "            _, right_buffer = cv2.imencode('.jpeg', right_img_np)\n",
    "            right_eye_image.value = right_buffer.tobytes()\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# When everything is done, release the capture\n",
    "finally:\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
