{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.9.19\n",
      "PyTotch: 2.3.1+cu118\n",
      "torch available:  True\n",
      "DEVICE:  NVIDIA GeForce GTX 1650 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import torch\n",
    "\n",
    "print(\"Python version: \",platform.python_version())\n",
    "print(\"PyTotch:\", torch.__version__)\n",
    "print(\"torch available: \", torch.cuda.is_available())\n",
    "print(\"DEVICE: \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudic/anaconda3/envs/ecg-classification/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/claudic/anaconda3/envs/ecg-classification/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from typing import Tuple, Union, Optional\n",
    "import math\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "os.environ[\"OPENCV_VIDEOIO_MSMF_ENABLE_HW_TRANSFORMS\"] = \"0\"\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_path': 'models/resnet18-e_20-d_10k.pth',\n",
    "    'image_size': (224, 224),\n",
    "    'class_names': ['close', 'open'],\n",
    "    'num_classes': 2,\n",
    "}\n",
    "\n",
    "# Constants\n",
    "EYE_VERTICAL_OFFSET = 35\n",
    "EYE_HORIZONTAL_OFFSET = 35\n",
    "\n",
    "# Define the device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def load_model(model_path: str, num_classes: int) -> nn.Module:\n",
    "    \"\"\"Load and prepare the model for inference.\"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model.to(device).eval()\n",
    "\n",
    "def load_face_detector():\n",
    "    \"\"\"Load the face detector model.\"\"\"\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    return mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "# Load models\n",
    "model = load_model(CONFIG['model_path'], CONFIG['num_classes'])\n",
    "\n",
    "# Image preprocessing\n",
    "def apply_clahe(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    equalized = clahe.apply(gray)\n",
    "    return cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv_gamma) * 255\n",
    "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def normalize_image(image):\n",
    "    return cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Apply a combination of techniques\n",
    "    image = normalize_image(image)\n",
    "    image = apply_clahe(image)\n",
    "    image = adjust_gamma(image, 1.2)  # Slightly increase brightness\n",
    "    return image\n",
    "\n",
    "# Define the transformation for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(CONFIG['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def eyes_detection(frame: np.ndarray, face_detection) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"Detect eyes in the given frame.\"\"\"\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image)\n",
    "    \n",
    "    height, width, _ = frame.shape\n",
    "    \n",
    "    lefteye_img = None\n",
    "    righteye_img = None\n",
    "    eyes_coor = None\n",
    "    \n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            x, y, w, h = int(bbox.xmin * width), int(bbox.ymin * height), int(bbox.width * width), int(bbox.height * height)\n",
    "            \n",
    "            # Estimate eye positions (you may need to adjust these ratios)\n",
    "            left_eye_x = int(x + w * 0.3)\n",
    "            right_eye_x = int(x + w * 0.7)\n",
    "            eye_y = int(y + h * 0.1)\n",
    "            \n",
    "            # eyes_coor = [(left_X1, left_X2), (right_X1, right_X2), (Y1, Y2)]\n",
    "            eyes_coor = [(left_eye_x - EYE_HORIZONTAL_OFFSET, left_eye_x + EYE_HORIZONTAL_OFFSET), \n",
    "                         (right_eye_x - EYE_HORIZONTAL_OFFSET, right_eye_x + EYE_HORIZONTAL_OFFSET),\n",
    "                         (eye_y - EYE_VERTICAL_OFFSET, eye_y + EYE_VERTICAL_OFFSET)]\n",
    "            \n",
    "            lefteye_img = frame[eyes_coor[2][0]:eyes_coor[2][1], eyes_coor[0][0]:eyes_coor[0][1]]\n",
    "                                \n",
    "            righteye_img = frame[eyes_coor[2][0]:eyes_coor[2][1], eyes_coor[1][0]:eyes_coor[1][1]]\n",
    "            \n",
    "            break  # Assume only one face for simplicity\n",
    "    \n",
    "    return lefteye_img, righteye_img, eyes_coor\n",
    "\n",
    "def predict_frame(frame: np.ndarray, face_detection) -> Tuple[int, Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"Predict the state of eyes in the given frame.\"\"\"\n",
    "    preprocessed_frame = preprocess_image(frame)\n",
    "    \n",
    "    left_frame, right_frame, eyes_coor = eyes_detection(preprocessed_frame, face_detection)\n",
    "\n",
    "    if left_frame is None or right_frame is None:\n",
    "        return \"Eyes not detected\", None, None, None\n",
    "\n",
    "    # Further preprocess the eye images\n",
    "    left_frame = preprocess_image(left_frame)\n",
    "    right_frame = preprocess_image(right_frame)\n",
    "\n",
    "    left_img = transform(Image.fromarray(left_frame)).unsqueeze(0).to(device)\n",
    "    right_img = transform(Image.fromarray(right_frame)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_left = model(left_img)\n",
    "        outputs_right = model(right_img)\n",
    "        _, predicted_left = torch.max(outputs_left, 1)\n",
    "        _, predicted_right = torch.max(outputs_right, 1)\n",
    "\n",
    "    result = predicted_left.item() + predicted_right.item()\n",
    "\n",
    "    return result, left_frame, right_frame, eyes_coor\n",
    "\n",
    "# Create widgets for displaying results\n",
    "result_text = widgets.Label(style={'font-size': '30px', 'color': 'red'})\n",
    "computeSpeed_text = widgets.Label(style={'font-size': '30px'})\n",
    "inputFPS_text = widgets.Label(style={'font-size': '30px'})\n",
    "left_eye_image = widgets.Image(format='png')\n",
    "right_eye_image = widgets.Image(format='png')\n",
    "origin_image = widgets.Image(format='png', width=320, height=240)\n",
    "\n",
    "right_eye_widget = widgets.VBox([\n",
    "    widgets.Label(value='  Right Eye', layout=widgets.Layout(align_items='center')),\n",
    "    right_eye_image\n",
    "])\n",
    "\n",
    "left_eye_widget = widgets.VBox([\n",
    "    widgets.Label(value='  Left Eye', layout=widgets.Layout(align_items='center')),\n",
    "    left_eye_image\n",
    "])\n",
    "\n",
    "origin_image_widget = widgets.VBox([\n",
    "    widgets.Label(value='    Camera frame', layout=widgets.Layout(align_items='center')),\n",
    "    origin_image\n",
    "])\n",
    "\n",
    "output_widget = widgets.HBox([\n",
    "    widgets.HBox([widgets.VBox([result_text, computeSpeed_text, inputFPS_text]),\n",
    "                  widgets.Label(value=' '),\n",
    "                widgets.HBox([left_eye_widget, right_eye_widget, origin_image_widget])\n",
    "    ])\n",
    "], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "def main():\n",
    "    \n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_V4L)\n",
    "    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 15.0)\n",
    "    \n",
    "    \n",
    "    # cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    display(output_widget)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    eyes_states_his = []\n",
    "    eyes_states = 1 # 1 for safe, 0 for danger\n",
    "    history_len = 5\n",
    "    \n",
    "    face_detection = load_face_detector()  # Create face detection object once\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            end_time = time.time()\n",
    "            compute_time = end_time - start_time\n",
    "            start_time = end_time\n",
    "            \n",
    "            num_opened_eyes, left_img, right_img, eyes_coor = predict_frame(frame, face_detection)\n",
    "            \n",
    "            if not eyes_coor is None:\n",
    "                cv2.rectangle(frame, (eyes_coor[0][0], eyes_coor[2][0]), (eyes_coor[0][1], eyes_coor[2][1]), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                cv2.rectangle(frame, (eyes_coor[1][0], eyes_coor[2][0]), (eyes_coor[1][1], eyes_coor[2][1]), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            _, frame_buffer = cv2.imencode('.png', frame)\n",
    "            origin_image.value = frame_buffer.tostring()\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(eyes_states_his) == history_len:\n",
    "                eyes_states_his = eyes_states_his[1:]\n",
    "                eyes_states_his.append(0 if num_opened_eyes == 0 else 1)\n",
    "                eyes_states = 0 if (eyes_states_his[0] == 0 and eyes_states_his[1] == 0 and eyes_states_his[2] == 0) or \\\n",
    "                                    (eyes_states_his[1] == 0 and eyes_states_his[2] == 0 and eyes_states_his[3] == 0) or \\\n",
    "                                    (eyes_states_his[2] == 0 and eyes_states_his[3] == 0 and eyes_states_his[4] == 0) else 1\n",
    "            else:\n",
    "                eyes_states_his.append(0 if num_opened_eyes ==0 else 1)\n",
    "            \n",
    "            fps = int(cap.get(5))\n",
    "            inputFPS_text.value = f'FPS: {fps}'\n",
    "            result_text.value = f'Predicted: {num_opened_eyes} eyes opened!'\n",
    "            computeSpeed_text.value = f'Compute Time: {compute_time:.3f} s'\n",
    "            \n",
    "            if left_img is not None:\n",
    "                if eyes_states:\n",
    "                    cv2.rectangle(left_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 255, 0), 5, cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.rectangle(left_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                \n",
    "                _, left_buffer = cv2.imencode('.png', left_img)\n",
    "                left_eye_image.value = left_buffer.tostring()\n",
    "\n",
    "            if right_img is not None:\n",
    "                if eyes_states:\n",
    "                    cv2.rectangle(right_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 255, 0), 5, cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.rectangle(right_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                    \n",
    "                _, right_buffer = cv2.imencode('.png', right_img)\n",
    "                right_eye_image.value = right_buffer.tostring()\n",
    "            \n",
    "            time.sleep(1/30)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    finally:\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2887a7e9b8b40feb4e93ebc26ede90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(VBox(children=(Label(value='Predicted: 2 eyes opened!'), Label(value='Compute Ti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722923297.204185    6205 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1722923297.240261    6317 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA GeForce GTX 1650 with Max-Q Design/PCIe/SSE2\n",
      "W0000 00:00:1722923297.243499    6312 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/tmp/ipykernel_6205/238218944.py:212: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  origin_image.value = frame_buffer.tostring()\n",
      "/tmp/ipykernel_6205/238218944.py:237: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  left_eye_image.value = left_buffer.tostring()\n",
      "/tmp/ipykernel_6205/238218944.py:246: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  right_eye_image.value = right_buffer.tostring()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
