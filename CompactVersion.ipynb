{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetsonsonnano661/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jetsonsonnano661/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1722396989.988731   10220 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6835c43f3c42ef979f1b245bd66350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(VBox(children=(Label(value=''), Label(value=''))), HBox(children=(VBox(children=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetsonsonnano661/.local/lib/python3.9/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from typing import Tuple, Union, Optional\n",
    "import math\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_path': 'models/resnet18-e_20-d_10k.pth',\n",
    "    'detector_path': 'models/detector.tflite',\n",
    "    'image_size': (224, 224),\n",
    "    'class_names': ['close', 'open'],\n",
    "    'num_classes': 2,\n",
    "}\n",
    "\n",
    "# Constants\n",
    "EYE_VERTICAL_OFFSET = 35\n",
    "EYE_HORIZONTAL_OFFSET = 35\n",
    "\n",
    "# Define the device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(model_path: str, num_classes: int) -> nn.Module:\n",
    "    \"\"\"Load and prepare the model for inference.\"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model.to(device).eval()\n",
    "\n",
    "def load_face_detector(detector_path: str) -> vision.FaceDetector:\n",
    "    \"\"\"Load the face detector model.\"\"\"\n",
    "    base_options = python.BaseOptions(model_asset_path=detector_path)\n",
    "    options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "    return vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "# Load models\n",
    "model = load_model(CONFIG['model_path'], CONFIG['num_classes'])\n",
    "detector = load_face_detector(CONFIG['detector_path'])\n",
    "\n",
    "# Image preprocessing\n",
    "'''\n",
    "def apply_histogram_equalization(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    equalized = cv2.equalizeHist(gray)\n",
    "    return cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def apply_white_balance(image):\n",
    "    result = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    avg_a = np.average(result[:, :, 1])\n",
    "    avg_b = np.average(result[:, :, 2])\n",
    "    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    return cv2.cvtColor(result, cv2.COLOR_LAB2BGR)\n",
    "'''\n",
    "def apply_clahe(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    equalized = clahe.apply(gray)\n",
    "    return cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv_gamma) * 255\n",
    "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def normalize_image(image):\n",
    "    return cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Apply a combination of techniques\n",
    "    image = normalize_image(image)\n",
    "    image = apply_clahe(image)\n",
    "    image = adjust_gamma(image, 1.2)  # Slightly increase brightness\n",
    "    return image\n",
    "\n",
    "# Define the transformation for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(CONFIG['image_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def _normalized_to_pixel_coordinates(\n",
    "    normalized_x: float, normalized_y: float, image_width: int,\n",
    "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "    \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
    "\n",
    "    def is_valid_normalized_value(value: float) -> bool:\n",
    "        return (value > 0 or math.isclose(0, value)) and (value < 1 or math.isclose(1, value))\n",
    "\n",
    "    if not (is_valid_normalized_value(normalized_x) and is_valid_normalized_value(normalized_y)):\n",
    "        return None\n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px\n",
    "\n",
    "def eyes_detection(frame: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"Detect eyes in the given frame.\"\"\"\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "    detection_result = detector.detect(image)\n",
    "    \n",
    "    image_np = np.copy(image.numpy_view())\n",
    "    height, width, _ = image_np.shape\n",
    "    \n",
    "    lefteye_img = righteye_img = None\n",
    "    \n",
    "    for detection in detection_result.detections:\n",
    "        keypoint_px_right = _normalized_to_pixel_coordinates(detection.keypoints[0].x, detection.keypoints[0].y, width, height)\n",
    "        keypoint_px_left = _normalized_to_pixel_coordinates(detection.keypoints[1].x, detection.keypoints[1].y, width, height)\n",
    "        \n",
    "        if keypoint_px_right:\n",
    "            righteye_img = image_np[keypoint_px_right[1] - EYE_VERTICAL_OFFSET:keypoint_px_right[1] + EYE_VERTICAL_OFFSET, keypoint_px_right[0] - EYE_HORIZONTAL_OFFSET:keypoint_px_right[0] + EYE_HORIZONTAL_OFFSET]\n",
    "        \n",
    "        if keypoint_px_left:\n",
    "            lefteye_img = image_np[keypoint_px_left[1] - EYE_VERTICAL_OFFSET:keypoint_px_left[1] + EYE_VERTICAL_OFFSET, keypoint_px_left[0] - EYE_HORIZONTAL_OFFSET:keypoint_px_left[0] + EYE_HORIZONTAL_OFFSET]\n",
    "        \n",
    "        break  # Assume only one face for simplicity\n",
    "    \n",
    "    return lefteye_img, righteye_img\n",
    "\n",
    "def predict_frame(frame: np.ndarray) -> Tuple[int, Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"Predict the state of eyes in the given frame.\"\"\"\n",
    "    \n",
    "    '''\n",
    "    left_frame, right_frame = eyes_detection(frame)\n",
    "\n",
    "    if left_frame is None or right_frame is None:\n",
    "        return \"Eyes not detected\", None, None\n",
    "\n",
    "    left_img = transform(Image.fromarray(left_frame)).unsqueeze(0).to(device)\n",
    "    right_img = transform(Image.fromarray(right_frame)).unsqueeze(0).to(device)\n",
    "    '''\n",
    "\n",
    "    preprocessed_frame = preprocess_image(frame)\n",
    "    \n",
    "    left_frame, right_frame = eyes_detection(preprocessed_frame)\n",
    "\n",
    "    if left_frame is None or right_frame is None:\n",
    "        return \"Eyes not detected\", None, None\n",
    "\n",
    "    # Further preprocess the eye images\n",
    "    left_frame = preprocess_image(left_frame)\n",
    "    right_frame = preprocess_image(right_frame)\n",
    "\n",
    "    left_img = transform(Image.fromarray(left_frame)).unsqueeze(0).to(device)\n",
    "    right_img = transform(Image.fromarray(right_frame)).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_left = model(left_img)\n",
    "        outputs_right = model(right_img)\n",
    "        _, predicted_left = torch.max(outputs_left, 1)\n",
    "        _, predicted_right = torch.max(outputs_right, 1)\n",
    "\n",
    "    result = predicted_left.item() + predicted_right.item()\n",
    "\n",
    "    return result, left_frame, right_frame\n",
    "\n",
    "# Create widgets for displaying results\n",
    "result_text = widgets.Label(style={'font-size': '30px', 'color': 'red'})\n",
    "computeSpeed_text = widgets.Label(style={'font-size': '30px'})\n",
    "# image_widget = widgets.Image(format='jpeg', width=640, height=480)\n",
    "left_eye_image = widgets.Image(format='png')\n",
    "right_eye_image = widgets.Image(format='png')\n",
    "\n",
    "right_eye_widget = widgets.VBox([\n",
    "    widgets.Label(value='Right Eye', layout=widgets.Layout(align_items='center')),\n",
    "    right_eye_image\n",
    "])\n",
    "\n",
    "left_eye_widget = widgets.VBox([\n",
    "    widgets.Label(value='Left Eye', layout=widgets.Layout(align_items='center')),\n",
    "    left_eye_image\n",
    "])\n",
    "\n",
    "output_widget = widgets.HBox([\n",
    "    # image_widget,\n",
    "    widgets.VBox([widgets.VBox([result_text, computeSpeed_text]),\n",
    "                widgets.HBox([left_eye_widget, right_eye_widget])\n",
    "    ])\n",
    "])\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_V4L2)\n",
    "    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        return\n",
    "\n",
    "    display(output_widget)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    eyes_states_his = []\n",
    "    eyes_states = 1 # 1 for safe, 0 for danger\n",
    "    history_len = 5\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            num_opened_eyes, left_img, right_img = predict_frame(frame)\n",
    "\n",
    "            end_time = time.time()\n",
    "            compute_time = end_time - start_time\n",
    "            start_time = end_time\n",
    "\n",
    "            if len(eyes_states_his) == history_len:\n",
    "                eyes_states_his = eyes_states_his[1:]\n",
    "                eyes_states_his.append(0 if num_opened_eyes == 0 else 1)\n",
    "                eyes_states = 0 if (eyes_states_his[0] == 0 and eyes_states_his[1] == 0 and eyes_states_his[2] == 0) or \\\n",
    "                                    (eyes_states_his[1] == 0 and eyes_states_his[2] == 0 and eyes_states_his[3] == 0) or \\\n",
    "                                    (eyes_states_his[2] == 0 and eyes_states_his[3] == 0 and eyes_states_his[4] == 0) else 1\n",
    "            else:\n",
    "                eyes_states_his.append(0 if num_opened_eyes ==0 else 1)\n",
    "            \n",
    "            result_text.value = f'Predicted: {num_opened_eyes} eyes opened!'\n",
    "            computeSpeed_text.value = f'Compute Time: {compute_time:.3f} s'\n",
    "            \n",
    "            \n",
    "            if left_img is not None:\n",
    "                if eyes_states:\n",
    "                    cv2.rectangle(left_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 255, 0), 5, cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.rectangle(left_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                \n",
    "                _, left_buffer = cv2.imencode('.png', left_img)\n",
    "                left_eye_image.value = left_buffer.tobytes()\n",
    "\n",
    "            if right_img is not None:\n",
    "                if eyes_states:\n",
    "                    cv2.rectangle(right_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 255, 0), 5, cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.rectangle(right_img, (0, 0), (EYE_VERTICAL_OFFSET * 2, EYE_HORIZONTAL_OFFSET * 2), (0, 0, 255), 5, cv2.LINE_AA)\n",
    "                    \n",
    "                _, right_buffer = cv2.imencode('.png', right_img)\n",
    "                right_eye_image.value = right_buffer.tobytes()\n",
    "\n",
    "            # time.sleep(0.1)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        # print(np.mean(np.array(speed_rec)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
